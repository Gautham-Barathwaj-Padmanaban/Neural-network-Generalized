{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Function to return the sigmoid values of all elements in a matrix\n",
    "\n",
    "def sigmoid(Z):\n",
    "    g=1/(1+np.exp(-Z))\n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to randomly initialize the parameters(weights) of the NN\n",
    "\n",
    "def randinit(input_size,hidden_size,n_hidden,n_labels):\n",
    "    \n",
    "    theta=[] #Setting up the empty list that will store the matrices of parameters(weights) for every layer of the NN\n",
    "    e=0.6 #range of initialization [-e,+e]\n",
    "\n",
    "    for i in range(n_hidden+1):\n",
    "\n",
    "            if i==0:\n",
    "                theta.append(((np.random.rand(hidden_size,input_size+1))*2*e)-e)#Parameters to move from input layer to first hidden layer\n",
    "\n",
    "            elif i==n_hidden:\n",
    "                theta.append(((np.random.rand(n_labels,hidden_size+1))*2*e)-e)#Parameters to move between intermediate hidden layers \n",
    "\n",
    "            else:\n",
    "                theta.append(((np.random.rand(hidden_size,hidden_size+1))*2*e)-e)#Parameters to move from last hidden layer to output layer\n",
    "\n",
    "    return(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to unroll parameters from list of matrices to a single row arra\n",
    "\n",
    "def unroll(M):\n",
    "\n",
    "    a=[]\n",
    "    arr=np.array([])\n",
    "    \n",
    "    for i in range(len(M)):\n",
    "        M[i]=M[i].flatten()\n",
    "        arr=np.concatenate((arr,M[i]))\n",
    "    a.append(arr.tolist())\n",
    "    a=np.array(a)\n",
    "\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(params,input_size,hidden_size,n_hidden,n_labels,use0):\n",
    "    \n",
    "    theta=[] \n",
    "    theta_grad=[] #Setting up an empty list to have arrays of derivatives of parameters for the layers of the NN\n",
    "    count=0 #Setting up a counter to track indexing for params when rolling\n",
    "    params_sum=0\n",
    "    \n",
    "    for i in range(n_hidden+1):\n",
    "        \n",
    "        if i==0:\n",
    "            theta.append(np.reshape(params[0:(hidden_size*(input_size+1))],(hidden_size,input_size+1)))\n",
    "            count=count+(hidden_size*(input_size+1))\n",
    "            \n",
    "        elif i==n_hidden:\n",
    "            theta.append(np.reshape(params[count:count+(n_labels*(hidden_size+1))],(n_labels,hidden_size+1)))\n",
    "            \n",
    "        else:\n",
    "            theta.append(np.reshape(params[count:count+(hidden_size*(hidden_size+1))],(hidden_size,hidden_size+1)))\n",
    "            count=count+(hidden_size*(hidden_size+1))\n",
    "                              \n",
    "        if use0==1: #This is only used inside the objective/cost function\n",
    "            \n",
    "            theta_grad.append(np.zeros(np.shape(theta[i]))) #Setting up the gradient matrix\n",
    "        \n",
    "            r=np.copy(theta[-1])\n",
    "            params_sum=params_sum+np.sum(np.power(r[:,1:],2)) #Adding all the squares of the parameters for regularization\n",
    "        \n",
    "    return(theta,theta_grad,params_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the cost/objective function and gradients for the neural network (NN)\n",
    "\n",
    "def computeCostandGrad(params,input_size,hidden_size,n_hidden,n_labels,X,y,lmbda):\n",
    "    \n",
    "    params=np.array(params) #Converting the parameters to a matrix\n",
    "    params=params.transpose() #Converting the row matrix to a column matrix\n",
    "    \n",
    "    m=np.shape(X)[0] #The number of training examples\n",
    "    J=0 #The Objective/Cost Function value \n",
    "    \n",
    "    #To reobtain the parameters/weights for the layers of the NN\n",
    "    (theta,theta_grad,params_sum)=roll(params,input_size,hidden_size,n_hidden,n_labels,1) \n",
    "    \n",
    "    I=np.identity(n_labels) #Setting up vectors for the labels for checking in output layer of the NN\n",
    "    \n",
    "    a=[] #Setting up an empty list to have arrays of activation values of the various layers of the NN\n",
    "    \n",
    "    #Finding the matrices of activation values\n",
    "    \n",
    "    for i in range(n_hidden+1):\n",
    "        \n",
    "        if i==0:\n",
    "            a.append(sigmoid((np.column_stack((np.ones(np.shape(X)[0]),X))).dot(theta[i].transpose()))) \n",
    "        else:\n",
    "            a.append(sigmoid((np.column_stack((np.ones(np.shape(a[i-1])[0]),a[i-1]))).dot(theta[i].transpose())))\n",
    "    \n",
    "    #Setting up the y matrix (output matrix of training data)\n",
    "    \n",
    "    yi=np.zeros((m,n_labels))\n",
    "    \n",
    "    for i in range(m):\n",
    "        yi[i,:]=I[y[i],:]\n",
    "    \n",
    "    #for i in range(m):\n",
    "        #if y[i]==0:\n",
    "            #yi[i,:]=I[n_labels-1,:]\n",
    "        #else:\n",
    "            #yi[i,:]=I[y[i]-1,:]\n",
    "        \n",
    "    #Finding the value of the objective/cost function\n",
    "    \n",
    "    J=(-1/m)*(np.sum((np.multiply(yi,np.log(a[-1])))+(np.multiply((1-yi),np.log((1-a[-1]))))))\n",
    "    \n",
    "    #Regularization of cost function\n",
    "    \n",
    "    J=J+((lmbda/(2*m))*params_sum)\n",
    "    \n",
    "    #Backpropagation to evaluate gradients\n",
    "    \n",
    "    delta=[] #Setting up empty list to store error matrices for the various layers of the NN\n",
    "    \n",
    "    for i in range(n_hidden+1):\n",
    "        \n",
    "        #Calculating the error values\n",
    "        \n",
    "        if i==0:\n",
    "            delta.append(a[-1]-yi)\n",
    "        else:\n",
    "            k=np.multiply(a[-1-i],(1-a[-1-i]))\n",
    "            delta.append(np.multiply(delta[i-1].dot(theta[-1-(i-1)]),np.column_stack((np.ones(np.shape(k)[0]),k))))\n",
    "            k=delta[-1]\n",
    "            delta[-1]=k[:,1:]\n",
    "        \n",
    "        #Computing the gradients (backpropagation)\n",
    "        \n",
    "        if i==n_hidden:\n",
    "            theta_grad[-1-i]=(1/m)*((delta[-1].transpose()).dot(np.column_stack((np.ones(np.shape(X)[0]),X))))\n",
    "        else:\n",
    "            theta_grad[-1-i]=(1/m)*((delta[-1].transpose()).dot(np.column_stack((np.ones(np.shape(a[i])[0]),a[i]))))\n",
    "            \n",
    "        #Regularization of gradient term\n",
    "        \n",
    "        k=np.copy(theta_grad[-1-i])\n",
    "        t=np.copy(theta[-1-i])\n",
    "        k[:,1:]=k[:,1:]+((lmbda/(m))*t[:,1:])\n",
    "        theta_grad[-1-i]=k\n",
    "            \n",
    "    #Unrolling gradients\n",
    "\n",
    "    grad=unroll(theta_grad)\n",
    "    \n",
    "    return(J,grad)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that the gradients are being computed properly\n",
    "#Important to check that back-propagation is running properly as it is prone to errors\n",
    "\n",
    "def gradientCheck(lmbda):\n",
    "    \n",
    "    #Creating a simplified mini-neural network with single hidden layer for the checking process\n",
    "    \n",
    "    input_size=2\n",
    "    hidden_size=4\n",
    "    n_labels=2\n",
    "    n_hidden=1\n",
    "    m=5 #Number of training examples for the NN\n",
    "    e=0.5 #Limits of the random input data\n",
    "    \n",
    "    #Setting up the learning parameters\n",
    "    \n",
    "    theta1=((np.random.rand(hidden_size,input_size+1))*2*e)-e\n",
    "    theta2=((np.random.rand(n_labels,hidden_size+1))*2*e)-e\n",
    "    theta_nn=np.concatenate((theta1.flatten(),theta2.flatten())) #Converting into a one dimensional array\n",
    "    \n",
    "    #Initializing random X and Y\n",
    "    \n",
    "    X=np.random.rand(m,input_size)\n",
    "    y=np.random.randint(0,n_labels,size=(m,1))\n",
    "    \n",
    "    (J_nn,grad_nn)=computeCostandGrad(theta_nn,input_size,hidden_size,n_hidden,n_labels,X,y,lmbda)\n",
    "    \n",
    "    epsilon=0.00001 #Constant for central difference\n",
    "    \n",
    "    grad_nn=grad_nn.transpose()\n",
    "    grad_cd=np.zeros((np.shape(grad_nn)))\n",
    "    theta_cd=np.copy(theta_nn)\n",
    "    \n",
    "    for i in range(np.shape(grad_nn)[0]):\n",
    "               \n",
    "            \n",
    "        #Finding the forward and backward terms for central differencing\n",
    "        \n",
    "        theta_cd[i]=theta_cd[i]+epsilon\n",
    "        (J_plus,not_used)=computeCostandGrad(theta_cd,input_size,hidden_size,n_hidden,n_labels,X,y,lmbda)\n",
    "        theta_cd[i]=theta_cd[i]-(2*epsilon)\n",
    "        (J_minus,not_used)=computeCostandGrad(theta_cd,input_size,hidden_size,n_hidden,n_labels,X,y,lmbda)\n",
    "      \n",
    "        grad_cd[i]=(J_plus-J_minus)/(2*epsilon) #Central Difference formula\n",
    "        \n",
    "        theta_cd=np.copy(theta_nn)\n",
    "    \n",
    "    print('''Gradients obtained for a trial mini neural network through the neural network and cetral difference\n",
    "    schemes have been compared as shown by below''')\n",
    "    a=np.concatenate((grad_nn,grad_cd),axis=1)\n",
    "    print(a)\n",
    "    print('\\n')\n",
    "    avr=(np.sum(grad_nn-grad_cd))/(np.shape(grad_nn)[0])\n",
    "    print('The average difference is ',avr)\n",
    "    print('\\n')\n",
    "    print('''An error of order lesser than 10^-9 is sufficient enough to hold confidence on the implementation\n",
    "    of the backpropagation code''')    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients obtained for a trial mini neural network through the neural network and cetral difference\n",
      "    schemes have been compared as shown by below\n",
      "[[-0.00170624 -0.00170624]\n",
      " [-0.43181396 -0.43181396]\n",
      " [-0.35662139 -0.35662139]\n",
      " [-0.0333351  -0.0333351 ]\n",
      " [ 0.16163533  0.16163533]\n",
      " [ 0.38530481  0.38530481]\n",
      " [-0.02790602 -0.02790602]\n",
      " [-0.31309449 -0.31309449]\n",
      " [ 0.30911627  0.30911627]\n",
      " [ 0.01668689  0.01668689]\n",
      " [-0.47553238 -0.47553238]\n",
      " [-0.26904231 -0.26904231]\n",
      " [ 0.18053257  0.18053257]\n",
      " [-0.26994559 -0.26994559]\n",
      " [ 0.02933429  0.02933429]\n",
      " [ 0.0834642   0.0834642 ]\n",
      " [-0.24701927 -0.24701927]\n",
      " [-0.28900878 -0.28900878]\n",
      " [-0.28378036 -0.28378036]\n",
      " [ 0.30066277  0.30066277]\n",
      " [ 0.31117     0.31117   ]\n",
      " [-0.54898338 -0.54898338]]\n",
      "\n",
      "\n",
      "The average difference is  6.41529522056e-13\n",
      "\n",
      "\n",
      "An error of order lesser than 10^-9 is sufficient enough to hold confidence on the implementation\n",
      "    of the backpropagation code\n"
     ]
    }
   ],
   "source": [
    "gradientCheck(5) #The regularization parameter is passed into the gradientCheck function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('DigitData.xlsx')\n",
    "\n",
    "#Separating X and Y data\n",
    "\n",
    "cols=data.shape[1] #.shape is a tuple hence you call with []. 0 for row size and 1 for column size\n",
    "X=data.iloc[:,0:cols-1] #iloc can be used to access the data in various ways\n",
    "y=data.iloc[:,cols-1:cols]\n",
    "X=np.array(X)\n",
    "y=np.array(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate random parameter values for the layers of NN\n",
    "\n",
    "theta=randinit(input_size,hidden_size,n_hidden,n_labels) \n",
    "theta_unroll=unroll(theta) #Unrolling the parameters to be used in the NN cost and gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10285,10285) and (1,10285) not aligned: 10285 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-535-a7f87d41ca3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#Neural Network Learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_unroll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mfmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomputeCostandGrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtheta_unroll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BFGS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mtheta_final_unrolled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mJ_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[1;31m#Value of cost function after optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MLRoboticsIOT\\Machine Learning Software\\python-3.6.3.amd64\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[1;32mD:\\MLRoboticsIOT\\Machine Learning Software\\python-3.6.3.amd64\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[0mgnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvecnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgnorm\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mgtol\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[0mpk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10285,10285) and (1,10285) not aligned: 10285 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Initializations\n",
    "\n",
    "#-----------------------------------------------USER PROVIDED---------------------------------------------------------------#\n",
    "hidden_size=25  #Number of nodes in the hidden layers\n",
    "n_hidden=1  #Number of hidden layers\n",
    "n_labels=10  #Number of labels in dataset for classification\n",
    "lmbda=5 #Regularization Parameter\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "input_size=np.shape(X)[1] #Number of nodes in the input layer\n",
    "\n",
    "#To generate random parameter values for the layers of NN\n",
    "\n",
    "theta=randinit(input_size,hidden_size,n_hidden,n_labels) \n",
    "theta_unroll=unroll(theta) #Unrolling the parameters to be used in the NN cost and gradient function\n",
    "#To find Value of cost function before optimization\n",
    "(J_init,grad_init)=computeCostandGrad(theta_unroll,input_size,hidden_size,n_hidden,n_labels,X,y,lmbda)\n",
    "\n",
    "#Neural Network Learning\n",
    "print(type(theta_unroll))\n",
    "fmin=minimize(fun=computeCostandGrad,x0=theta_unroll,args=(input_size,hidden_size,n_hidden,n_labels,X,y,lmbda),method='BFGS',jac=True)\n",
    "theta_final_unrolled=fmin.x\n",
    "J_final=fmin.fun #Value of cost function after optimization\n",
    "\n",
    "\n",
    "print('The optimization process is complete\\n')\n",
    "print('The initial value of the cost/objective function =',J_init)\n",
    "print('The final value of the cost/objective function =',J_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the optimized parameters/weights of the NN layers\n",
    "(theta_opt,not_used,not_used)=roll(theta_final_unrolled,input_size,hidden_size,n_hidden,n_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the neural network is 96.39999999999999 %\n"
     ]
    }
   ],
   "source": [
    "#To predict the accuracy of the NN by checking with given input data\n",
    "\n",
    "m=np.shape(X)[0] #The number of training examples\n",
    "\n",
    "for i in range(len(theta_opt)):   \n",
    "    k=np.copy(theta_opt[i])\n",
    "    k=k.transpose()\n",
    "    \n",
    "    if i==0:     \n",
    "        h=sigmoid(np.column_stack((np.ones(np.shape(X)[0]),X)).dot(k))\n",
    "    else:\n",
    "        h=sigmoid(np.column_stack((np.ones(np.shape(h)[0]),h)).dot(k))\n",
    "\n",
    "y_predict=np.matrix(np.argmax(h,axis=1))\n",
    "y_predict=y_predict.transpose()\n",
    "accuracy=(1-(((np.count_nonzero(y_predict-y)))/len(y_predict)))*100\n",
    "print('The accuracy of the neural network is',accuracy,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights/parameters for the layers of the neural network are stored in theta_opt, which is a list of arrays. The first array corresponds to the first layer and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [9]\n",
      " [9]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
